

Web Scraping part 3: Scaling up
========================================================
width: 1200
author: Rolf Fredheim and Aiora Zabala
date: University of Cambridge
font-family: 'Rockwell'
04/03/2014

Catch up
==================

Slides from week 1: http://quantifyingmemory.blogspot.com/2014/02/web-scraping-basics.html

Slides from week 2: http://quantifyingmemory.blogspot.com/2014/02/web-scraping-part2-digging-deeper.html

Today we will:
========================================================

- Revision: loop over a scraper 
- Collect relevant URLs
- Download files
- Look at copyright issues
- Basic text manipulation in R
- Introduce APIs (subject of the final session)

Get the docs:
http://fredheir.github.io/WebScraping/Lecture3/p3.html

http://fredheir.github.io/WebScraping/Lecture3/p3.Rpres

http://fredheir.github.io/WebScraping/Lecture3/p3.r


Digital data collection
=======================

- **Devise a means of accessing data**
- Retrieve that data
- **tabulate and store the data**

Today we focus less on interacting with JSON or HTML; more on automating and repeating


Using a scraper
===============
<img src=p3_1.png?raw=true" style="height: 100%;"/>


In code
===================
Remember this from last week?

```r
bbcScraper <- function(url){
  SOURCE <-  getURL(url,encoding="UTF-8")
  PARSED <- htmlParse(SOURCE,encoding="UTF-8")
  title=xpathSApply(PARSED, "//h1[@class='story-header']",xmlValue)
  date=as.character(xpathSApply(PARSED, "//meta[@name='OriginalPublicationDate']/@content"))
  if (is.null(date))    date <- NA
  if (is.null(title))    title <- NA
  return(c(title,date))
}
```

We want to scale up
======================
<img src=p3_2.png?raw=true" style="height: 100%;"/>




How?
============

- Loop and rbind
- sapply
- sink into database

sqlite in R: http://sandymuspratt.blogspot.co.uk/2012/11/r-and-sqlite-part-1.html

Loop
==============
type:sq

```r
require(RCurl)
require(XML)
urls <- c("http://www.bbc.co.uk/news/business-26414285","http://www.bbc.co.uk/news/uk-26407840","http://www.bbc.co.uk/news/world-asia-26413101","http://www.bbc.co.uk/news/uk-england-york-north-yorkshire-26413963")
results=NULL
for (url in urls){
  newEntry <- bbcScraper(url)
  results <- rbind(results,newEntry)
}
data.frame(results) #ignore the warning
```

```
                                                       X1
1 Russian rouble hits new low against the dollar and euro
2                    'Two Together Railcard' goes on sale
3            Australia: Snake eats crocodile after battle
4   Missing Megan Roberts: Police find body in River Ouse
                   X2
1 2014/03/03 16:06:54
2 2014/03/03 02:02:11
3 2014/03/03 08:34:05
4 2014/03/03 06:59:50
```
Still to do: fix column names

Disadvantage of loop
=====================
type:sq2
left:80
copying data this way is inefficient:

```r
temp=NULL
#1st loop
temp <- rbind(temp,results[1,])
temp
```

```
     [,1]                                                     
[1,] "Russian rouble hits new low against the dollar and euro"
     [,2]                 
[1,] "2014/03/03 16:06:54"
```

```r
#2nd loop
temp <- rbind(temp,results[2,])

#3d loop
temp <- rbind(temp,results[3,])

#4th loop
temp <- rbind(temp,results[4,])
temp
```

```
     [,1]                                                     
[1,] "Russian rouble hits new low against the dollar and euro"
[2,] "'Two Together Railcard' goes on sale"                   
[3,] "Australia: Snake eats crocodile after battle"           
[4,] "Missing Megan Roberts: Police find body in River Ouse"  
     [,2]                 
[1,] "2014/03/03 16:06:54"
[2,] "2014/03/03 02:02:11"
[3,] "2014/03/03 08:34:05"
[4,] "2014/03/03 06:59:50"
```

=============
In each case we are copying the whole table in order to add a single line. 
- this is slow
- need to keep two copied in memory (means we can only ever use at most half of computer's RAM)

sapply
===========
A bit more efficient.

Takes a vector.
Applies a formula to each item in the vector:


```r
dat <- c(1,2,3,4,5)
sapply(dat,function(x) x*2)
```

```
[1]  2  4  6  8 10
```

syntax: sapply(data,function)
function: can be your own function, or a standard one:


```r
sapply(dat,sqrt)
```

```
[1] 1.000000 1.414214 1.732051 2.000000 2.236068
```

in our case:
========================
type:sq

```r
urls <- c("http://www.bbc.co.uk/news/business-26414285","http://www.bbc.co.uk/news/uk-26407840","http://www.bbc.co.uk/news/world-asia-26413101","http://www.bbc.co.uk/news/uk-england-york-north-yorkshire-26413963")

sapply(urls,bbcScraper)
```

```
     http://www.bbc.co.uk/news/business-26414285              
[1,] "Russian rouble hits new low against the dollar and euro"
[2,] "2014/03/03 16:06:54"                                    
     http://www.bbc.co.uk/news/uk-26407840 
[1,] "'Two Together Railcard' goes on sale"
[2,] "2014/03/03 02:02:11"                 
     http://www.bbc.co.uk/news/world-asia-26413101 
[1,] "Australia: Snake eats crocodile after battle"
[2,] "2014/03/03 08:34:05"                         
     http://www.bbc.co.uk/news/uk-england-york-north-yorkshire-26413963
[1,] "Missing Megan Roberts: Police find body in River Ouse"           
[2,] "2014/03/03 06:59:50"                                             
```

we don't really want the data in this format. We can reshape it, or use a related function:

=================

ldply: For each element of a list, apply function then combine results into a data frame.


```r
require(plyr)
dat <- ldply(urls,bbcScraper)
dat
```

```
                                                       V1
1 Russian rouble hits new low against the dollar and euro
2                    'Two Together Railcard' goes on sale
3            Australia: Snake eats crocodile after battle
4   Missing Megan Roberts: Police find body in River Ouse
                   V2
1 2014/03/03 16:06:54
2 2014/03/03 02:02:11
3 2014/03/03 08:34:05
4 2014/03/03 06:59:50
```



Task
===============
type:section
- Scrape ten BBC news articles
- Put them in a data frame


Link harvesting
====================

Entering URLs by hand is tedious

We can speed this up by automating the collection of links

How can we find relevant links?
- ?
- Scraping URLs in a search result
- those on the front page



Search result
============
Go to page, make a search, repeat that query in R

Press next page to get pagination pattern. E.g:
http://www.bbc.co.uk/search/news/?page=2&q=Russia

Task
==========
type:section
Can you collect the links from that page?
Can you restrict it to the search results (find the right div)

One solution
================
type:sq1
All links

```r
url="http://www.bbc.co.uk/search/news/?page=3&q=Russia"
SOURCE <-  getURL(url,encoding="UTF-8")
PARSED <- htmlParse(SOURCE)
xpathSApply(PARSED, "//a/@href")
```

filtered links:

```r
unique(xpathSApply(PARSED, "//a[@class='title linktrack-title']/@href"))
#OR xpathSApply(PARSED, "//div[@id='news content']/@href")
```

Why 'unique'?

Another option: xpathSApply(PARSED, "//div[@id='news content']/@href")

Create a table
=====================
type:sq1

```r
require(plyr)
targets <- unique(xpathSApply(PARSED, "//a[@class='title linktrack-title']/@href"))
results <- ldply(targets[1:5],bbcScraper) #limiting it to first five pages
results
```

```
data frame with 0 columns and 0 rows
```


Scale up further
==============
Account for pagination by writing a scraper that searches:

http://www.bbc.co.uk/search/news/?page=3&q=Russia
http://www.bbc.co.uk/search/news/?page=4&q=Russia
http://www.bbc.co.uk/search/news/?page=5&q=Russia

etc.

hint: use paste or paste0

Let's not do this in class though, and give the BBC's servers some peace


Copyright
======================
type:sq

Reading
- http://about.bloomberglaw.com/practitioner-contributions/legal-issues-raised-by-the-use-of-web-crawling-and-scraping-tools-for-analytics-purposes/
- http://www.theguardian.com/media-tech-law/tangled-web-of-copyright-law
- http://matthewsag.com/googlebooks-decision-fair-use/
- http://www.bbc.co.uk/news/technology-26187730
- http://matthewsag.com/anotherbestpracitcescode/
- http://www.ipo.gov.uk//response-2011-copyright-final.pdf
- http://www.arl.org/storage/documents/publications/code-of-best-practices-fair-use.pdf

> One case involved an online activist who scraped the MIT website and ultimately downloaded millions of academic articles.  This guy is now free on bond, but faces dozens of years in prison and $1 million if convicted.

===================
> PubMed	Central	UK	has	strong	provisions	against	automated	and	systematic	download	of articles:		Crawlers	 and	 other	 automated	 processes	may	NOT	 be	 used	 to	 systematically	 retrieve	batches	 of	 articles	 from	 the	 UKPMC	 web	 site.	 Bulk	 downloading	 of	 articles	 from	 the	main	UKPMC	web	site,	in	any	way,	is	prohibited	because	of	copyright	restrictions.

--http://www.technollama.co.uk/wp-content/uploads/2013/04/Data-Mining-Paper.pdf

Rough guidelines
=======================
type:sq
I don't know IP law. So all the below are no more than guidelines

Most legal cases relate to copying and republishing of information for profit (see article1 above)

- Don't cause damage (through republishing, taking-down servers, prevention of profit)
- Make sure your use is 'transformative'
- Read the TOCs

In 2013 Google Books was found to be 'fair use'
- Judgement good news for scholars and libraries

Archiving and downloading 
- databases can be used to 'facilitate non-consumptive research'

Distinction made between freely available content, and that behind a pay-wall, requiring to accept conditions, etc.

Downloading
===========
type:sq
Don't download loads of journal articles. Just don't

If you need/want to download something requiring authentication, be careful.

-attach cookie
Packages: httr

Many resources on how to do this, many situations in which this is totally OK. It is not hard, but often quite shady. If you need this for your work, consider the explanations below, and exercise restraint. 
<small>
- http://stackoverflow.com/questions/13204503
- http://stackoverflow.com/questions/10213194
- http://stackoverflow.com/questions/16118140
- http://stackoverflow.com/questions/15853204
- http://stackoverflow.com/questions/19074359
- http://stackoverflow.com/questions/8510528
- http://stackoverflow.com/questions/9638451
- http://stackoverflow.com/questions/2388974
</small>

Principles of downloading
=====================

Function: download.file(url,destfile)

destfile = filename on disk

option: mode="wb"

Often files downloaded are corrupted. Setting mode ="wb" prevents "\n" causing havoc, e.g. with image files


Example
===============

http://lib.ru

Library of Russian language copyright-free works. (like Gutenberg)

This is a 1747 translation of Hamlet into Russian

Run this in your terminal:

```r
url <- "http://lib.ru/SHAKESPEARE/hamlet8.pdf"
download.file(url,"hamlet.pdf",mode="wb")
```

navigate to your working directory, and you should have the pdf there

Automating downloads
=================
type:sq1
As with the newspaper articles, downloads are facilitated by getting the right links. Let's search for pdf files and download the first ten results:


```r
url <- "http://lib.ru/GrepSearch?Search=pdf"

SOURCE <-  getURL(url,encoding="UTF-8") # Specify encoding when dealing with non-latin characters
PARSED <- htmlParse(SOURCE)
links <- (xpathSApply(PARSED, "//a/@href"))
links[grep("pdf",links)][1]
```

```
                           href 
"/ANEKDOTY/REZNIK/perewody.pdf" 
```

```r
links <- paste0("http://lib.ru",links[grep("pdf",links)])
links[1]
```

```
[1] "http://lib.ru/ANEKDOTY/REZNIK/perewody.pdf"
```

Can you write a loop to download the first ten links?


Solutions
========================

```r
for (i in 1:10){
  parts <- unlist(str_split(links[i],"/"))
  outName <- parts[length(parts)]
  print(outName)
  download.file(links[i],outName)
}
```


String manipulation in R
==============
type:sq2
Hardest part of task above: meaningful filenames

Done using str_split

Top string manipulation functions:
<small>
- grep
- gsub
- str_split (library: stringr)
- paste
- nchar
- tolower (also  toupper, capitalize)
- str_trim (library: stringr)
- Encoding <a href="http://quantifyingmemory.blogspot.com/2013/01/r-and-foreign-characters.html"> read here<a>
</small>

Reading: 
<small>
- http://en.wikibooks.org/wiki/R_Programming/Text_Processing
- http://chemicalstatistician.wordpress.com/2014/02/27/useful-functions-in-r-for-manipulating-text-data/
</small>

What do they do: grep
=====================
type:sq1
Grep + regex: find stuff

```r
grep("SHAKESPEARE",links)
```

```
 [1]  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115
[18] 116 117 118
```

```r
links[grep("SHAKESPEARE",links)] #or: grep("SHAKESPEARE",links,value=T)
```

```
 [1] "http://lib.ru/SHAKESPEARE/shks_romeo7.pdf"        
 [2] "http://lib.ru/SHAKESPEARE/to_be_or_not_to_be.pdf" 
 [3] "http://lib.ru/SHAKESPEARE/gamlet_mihalowskij.pdf" 
 [4] "http://lib.ru/SHAKESPEARE/to_be_or_not_to_be3.pdf"
 [5] "http://lib.ru/SHAKESPEARE/to_be_or_not_to_be4.pdf"
 [6] "http://lib.ru/SHAKESPEARE/to_be_or_not_to_be5.pdf"
 [7] "http://lib.ru/SHAKESPEARE/to_be_or_not_to_be6.pdf"
 [8] "http://lib.ru/SHAKESPEARE/hamlet8.pdf"            
 [9] "http://lib.ru/SHAKESPEARE/hamlet9.pdf"            
[10] "http://lib.ru/SHAKESPEARE/hamlet10.pdf"           
[11] "http://lib.ru/SHAKESPEARE/hamlet11.pdf"           
[12] "http://lib.ru/SHAKESPEARE/shks_hamlet13.pdf"      
[13] "http://lib.ru/SHAKESPEARE/shks_hamlet14.pdf"      
[14] "http://lib.ru/SHAKESPEARE/shks_hamlet17.pdf"      
[15] "http://lib.ru/SHAKESPEARE/shks_hamlet22.pdf"      
[16] "http://lib.ru/SHAKESPEARE/shks_hamlet24.pdf"      
[17] "http://lib.ru/SHAKESPEARE/shks_henry_IV_5.pdf"    
[18] "http://lib.ru/SHAKESPEARE/shks_henry_V_2.pdf"     
[19] "http://lib.ru/SHAKESPEARE/shks_henryVI_3_2.pdf"   
[20] "http://lib.ru/SHAKESPEARE/henry8_2.pdf"           
```


Grep 2
============
type:sq

useful options: 
invert=T : get all non-matches
ignore.case=T : what it says on the box
value = T : return values rather than positions

Especially good with regex for partial matches:

```r
grep("hamlet*",links,value=T)[1]
```

```
[1] "http://lib.ru/LITRA/SUMAROKOW/hamlet8.pdf"
```


Regex
========
Check out 
- ?regex 
- http://www.rexegg.com/regex-quickstart.html

Can match beginning or end of word, e.g.:

```r
grep("stalin",c("stalin","stalingrad"),value=T)
```

```
[1] "stalin"     "stalingrad"
```

```r
grep("stalin\\b",c("stalin","stalingrad"),value=T)
```

```
[1] "stalin"
```

What do they do: gsub
=====================

```r
author <- "By Rolf Fredheim"
gsub("By ","",author)
```

```
[1] "Rolf Fredheim"
```

```r
gsub("Rolf Fredheim","Tom",author)
```

```
[1] "By Tom"
```

Gsub can also use regex

str_split
==============
type:sq

- Manipulating URLs
- Editing time stamps, etc

syntax: str_split(inputString,pattern)
returns a list





























```
Error in eval(expr, envir, enclos) : could not find function "str_split"
```
